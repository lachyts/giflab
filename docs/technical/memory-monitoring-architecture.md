# Memory Monitoring Infrastructure Architecture

This document provides comprehensive technical documentation for the memory monitoring and pressure management system implemented in Phase 3.1 of the critical code review resolution project.

## Overview

The memory monitoring infrastructure provides automatic memory pressure detection, cache eviction, and system protection to prevent memory exhaustion during GIF processing operations.

### Key Design Principles

1. **Proactive Protection**: Prevent memory exhaustion before it occurs
2. **Conservative Defaults**: Safe memory thresholds with hysteresis to prevent oscillation  
3. **Cross-Platform Compatibility**: Works across different operating systems
4. **Thread Safety**: Safe for concurrent operations and multi-threaded processing
5. **Performance Focus**: <1% monitoring overhead impact
6. **Integration Ready**: Seamless integration with existing monitoring and alerting systems

---

## Architecture Overview

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Memory Monitoring Infrastructure                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                  System Memory Monitor                         â”‚ â”‚
â”‚  â”‚                                                                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ Cross-Platform  â”‚  â”‚ MemoryStats     â”‚  â”‚   Thread-Safe   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ psutil-based    â”‚  â”‚ Collection      â”‚  â”‚   Operations    â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ Implementation  â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ + Fallbacks     â”‚  â”‚ â€¢ Total Memory  â”‚  â”‚ â€¢ RLock-based   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚ â€¢ Available     â”‚  â”‚   synchronizationâ”‚ â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚ â€¢ Process Memoryâ”‚  â”‚ â€¢ Atomic updatesâ”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚                                â”‚
â”‚                                     â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                Memory Pressure Manager                         â”‚ â”‚
â”‚  â”‚                                                                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚   Pressure      â”‚  â”‚   Eviction      â”‚  â”‚  Hysteresis     â”‚  â”‚ â”‚
â”‚  â”‚  â”‚   Detection     â”‚  â”‚   Policies      â”‚  â”‚  Prevention     â”‚  â”‚ â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Threshold     â”‚  â”‚ â€¢ Conservative  â”‚  â”‚ â€¢ Cooldown      â”‚  â”‚ â”‚
â”‚  â”‚  â”‚   monitoring    â”‚  â”‚   Policy        â”‚  â”‚   periods       â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Alert levels  â”‚  â”‚ â€¢ Progressive   â”‚  â”‚ â€¢ Delta-based   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Event streams â”‚  â”‚   targeting     â”‚  â”‚   thresholds    â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚                                â”‚
â”‚                                     â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                 Cache Memory Tracker                           â”‚ â”‚
â”‚  â”‚                                                                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ Memory Usage    â”‚  â”‚ Cache Registry  â”‚  â”‚ Eviction Target â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ Aggregation     â”‚  â”‚ Management      â”‚  â”‚ Calculation     â”‚  â”‚ â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Per-cache     â”‚  â”‚ â€¢ Registration  â”‚  â”‚ â€¢ Size-based    â”‚  â”‚ â”‚
â”‚  â”‚  â”‚   tracking      â”‚  â”‚   callbacks     â”‚  â”‚   targeting     â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Total memory  â”‚  â”‚ â€¢ Eviction      â”‚  â”‚ â€¢ Priority      â”‚  â”‚ â”‚
â”‚  â”‚  â”‚   summation     â”‚  â”‚   coordination  â”‚  â”‚   algorithms    â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Integration Layer                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Alert Manager   â”‚  â”‚   CLI Status    â”‚  â”‚ Metrics Integration â”‚   â”‚
â”‚  â”‚ Integration     â”‚  â”‚   Reporting     â”‚  â”‚                     â”‚   â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚ â€¢ Real-time stats   â”‚   â”‚
â”‚  â”‚ â€¢ Memory        â”‚  â”‚ â€¢ Real-time     â”‚  â”‚ â€¢ Historical data   â”‚   â”‚
â”‚  â”‚   pressure      â”‚  â”‚   memory usage  â”‚  â”‚ â€¢ Performance       â”‚   â”‚ â”‚
â”‚  â”‚   alerts        â”‚  â”‚ â€¢ Pressure      â”‚  â”‚   metrics           â”‚   â”‚
â”‚  â”‚ â€¢ Event         â”‚  â”‚   indicators    â”‚  â”‚ â€¢ Alert integrationâ”‚   â”‚ â”‚
â”‚  â”‚   streaming     â”‚  â”‚ â€¢ Cache status  â”‚  â”‚                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Components

### 1. System Memory Monitor

**Purpose**: Cross-platform memory tracking and statistics collection  
**Location**: `src/giflab/monitoring/memory_monitor.py`

```python
@dataclass
class MemoryStats:
    """Memory statistics data structure."""
    total_memory: int           # Total system memory (bytes)
    available_memory: int       # Available system memory (bytes)
    process_memory: int         # Current process memory (bytes) 
    memory_percent: float       # Memory usage percentage (0.0-1.0)
    timestamp: float           # Collection timestamp

class SystemMemoryMonitor:
    """Cross-platform system memory monitoring."""
    
    def __init__(self, update_interval: float = 5.0):
        self.update_interval = update_interval
        self._stats_cache = None
        self._last_update = 0.0
        self._lock = threading.RLock()
    
    def get_memory_stats(self) -> MemoryStats:
        """Get current memory statistics with caching."""
        with self._lock:
            current_time = time.time()
            
            if (self._stats_cache is None or 
                current_time - self._last_update > self.update_interval):
                
                self._stats_cache = self._collect_memory_stats()
                self._last_update = current_time
            
            return self._stats_cache
    
    def _collect_memory_stats(self) -> MemoryStats:
        """Collect memory statistics via psutil."""
        try:
            import psutil
            
            # System memory
            memory = psutil.virtual_memory()
            
            # Process memory
            process = psutil.Process()
            process_memory = process.memory_info().rss
            
            return MemoryStats(
                total_memory=memory.total,
                available_memory=memory.available,
                process_memory=process_memory,
                memory_percent=memory.percent / 100.0,
                timestamp=time.time()
            )
            
        except ImportError:
            # Fallback implementation without psutil
            return self._fallback_memory_stats()
    
    def _fallback_memory_stats(self) -> MemoryStats:
        """Fallback memory collection when psutil unavailable."""
        # Platform-specific fallback implementations
        # Returns basic memory information or estimates
        pass
```

**Key Features**:
- **Caching**: Configurable update intervals (default 5.0s) to reduce overhead
- **Thread Safety**: RLock-based synchronization for concurrent access
- **Fallback Support**: Works without psutil dependency via platform-specific methods
- **Performance**: <1ms collection overhead with caching

### 2. Memory Pressure Manager

**Purpose**: Automatic pressure detection and eviction management  
**Location**: `src/giflab/monitoring/memory_monitor.py`

```python
class MemoryPressureManager:
    """Manages memory pressure detection and automatic eviction."""
    
    def __init__(self, config: dict):
        self.thresholds = config["thresholds"]
        self.eviction_targets = config["eviction_targets"] 
        self.hysteresis = config["hysteresis"]
        
        self._memory_monitor = SystemMemoryMonitor(
            update_interval=config.get("update_interval", 5.0)
        )
        self._last_eviction_time = 0.0
        self._eviction_callbacks = []
    
    def check_memory_pressure(self) -> dict:
        """Check current memory pressure level."""
        stats = self._memory_monitor.get_memory_stats()
        
        pressure_info = {
            "level": "normal",
            "usage_percent": stats.memory_percent,
            "should_evict": False,
            "target_reduction": 0.0,
            "stats": stats
        }
        
        # Determine pressure level
        if stats.memory_percent >= self.thresholds["emergency"]:
            pressure_info.update({
                "level": "emergency",
                "should_evict": True,
                "target_reduction": self.eviction_targets["emergency"]
            })
        elif stats.memory_percent >= self.thresholds["critical"]:
            pressure_info.update({
                "level": "critical", 
                "should_evict": True,
                "target_reduction": self.eviction_targets["critical"]
            })
        elif stats.memory_percent >= self.thresholds["warning"]:
            pressure_info.update({
                "level": "warning",
                "should_evict": self._should_evict_with_hysteresis(),
                "target_reduction": self.eviction_targets["warning"]
            })
        
        return pressure_info
    
    def _should_evict_with_hysteresis(self) -> bool:
        """Implement hysteresis to prevent eviction oscillation."""
        current_time = time.time()
        cooldown_period = self.hysteresis["eviction_cooldown"]
        
        # Prevent eviction if in cooldown period
        if current_time - self._last_eviction_time < cooldown_period:
            return False
        
        return True
    
    def register_eviction_callback(self, callback: callable):
        """Register callback for memory pressure eviction."""
        self._eviction_callbacks.append(callback)
    
    def trigger_eviction_if_needed(self) -> dict:
        """Check pressure and trigger eviction if needed."""
        pressure_info = self.check_memory_pressure()
        
        if pressure_info["should_evict"]:
            evicted_bytes = 0
            
            for callback in self._eviction_callbacks:
                try:
                    bytes_freed = callback(pressure_info["target_reduction"])
                    evicted_bytes += bytes_freed
                except Exception as e:
                    logger.warning(f"Eviction callback failed: {e}")
            
            self._last_eviction_time = time.time()
            
            pressure_info["evicted_bytes"] = evicted_bytes
        
        return pressure_info
```

**Key Features**:
- **Progressive Thresholds**: Warning (70%), Critical (80%), Emergency (95%)
- **Hysteresis Prevention**: Cooldown periods prevent oscillation between eviction/loading
- **Callback System**: Pluggable eviction targets via callback registration
- **Automatic Triggering**: Can be called manually or via timer-based automation

### 3. Conservative Eviction Policy

**Purpose**: Smart eviction algorithms that balance performance and memory safety  
**Location**: `src/giflab/monitoring/memory_monitor.py`

```python
class ConservativeEvictionPolicy:
    """Conservative eviction policy with hysteresis prevention."""
    
    def __init__(self, config: dict):
        self.enable_delta = config["hysteresis"]["enable_delta"]
        self.eviction_cooldown = config["hysteresis"]["eviction_cooldown"]
        self._last_pressure_level = "normal"
        self._last_eviction_time = 0.0
    
    def should_evict(self, current_pressure: float, target_pressure: float) -> bool:
        """Determine if eviction should occur."""
        current_time = time.time()
        
        # Cooldown period check
        if current_time - self._last_eviction_time < self.eviction_cooldown:
            return False
        
        # Hysteresis: only evict if pressure significantly above target
        pressure_delta = current_pressure - target_pressure
        
        return pressure_delta > self.enable_delta
    
    def calculate_eviction_target(self, cache_memory_usage: int, 
                                 target_reduction: float) -> int:
        """Calculate how much memory to evict."""
        target_bytes = int(cache_memory_usage * target_reduction)
        
        # Conservative minimum: at least 10MB or 5% of cache, whichever is smaller
        min_eviction = min(10 * 1024 * 1024, int(cache_memory_usage * 0.05))
        
        return max(target_bytes, min_eviction)
    
    def select_eviction_candidates(self, cache_registry: dict, 
                                   target_bytes: int) -> list:
        """Select which cache entries to evict."""
        candidates = []
        
        # Prioritize by cache type and age
        for cache_id, cache_info in cache_registry.items():
            priority_score = self._calculate_eviction_priority(cache_info)
            candidates.append((cache_id, cache_info, priority_score))
        
        # Sort by priority (higher score = evict first)
        candidates.sort(key=lambda x: x[2], reverse=True)
        
        # Select candidates until target reached
        selected = []
        bytes_selected = 0
        
        for cache_id, cache_info, priority in candidates:
            if bytes_selected >= target_bytes:
                break
            
            selected.append((cache_id, cache_info))
            bytes_selected += cache_info.get("memory_usage", 0)
        
        return selected
    
    def _calculate_eviction_priority(self, cache_info: dict) -> float:
        """Calculate eviction priority score (higher = evict first)."""
        # Factors: age, access frequency, memory usage, cache type
        age_factor = time.time() - cache_info.get("last_access", 0)
        size_factor = cache_info.get("memory_usage", 0) / (1024 * 1024)  # MB
        access_factor = 1.0 / max(cache_info.get("access_count", 1), 1)
        
        # Cache type priorities (higher = evict first)
        type_priorities = {
            "validation_cache": 3.0,  # Least critical
            "resize_cache": 2.0,      # Medium priority
            "frame_cache": 1.0        # Most critical (evict last)
        }
        
        type_factor = type_priorities.get(cache_info.get("cache_type", "unknown"), 2.0)
        
        # Weighted priority calculation
        priority = (age_factor * 0.4 + 
                   size_factor * 0.3 + 
                   access_factor * 0.2 + 
                   type_factor * 0.1)
        
        return priority
```

### 4. Cache Memory Tracker

**Purpose**: Track memory usage across all cache types with eviction coordination  
**Location**: `src/giflab/monitoring/memory_monitor.py`

```python
class CacheMemoryTracker:
    """Thread-safe cache memory tracking and aggregation."""
    
    def __init__(self):
        self._cache_registry = {}
        self._lock = threading.RLock()
        
    def register_cache(self, cache_id: str, cache_info: dict):
        """Register a cache for memory tracking."""
        with self._lock:
            self._cache_registry[cache_id] = {
                **cache_info,
                "registration_time": time.time(),
                "last_update": time.time()
            }
    
    def update_cache_usage(self, cache_id: str, memory_usage: int, 
                          additional_stats: dict = None):
        """Update memory usage statistics for a cache."""
        with self._lock:
            if cache_id in self._cache_registry:
                self._cache_registry[cache_id].update({
                    "memory_usage": memory_usage,
                    "last_update": time.time(),
                    **(additional_stats or {})
                })
    
    def get_total_cache_memory(self) -> int:
        """Get total memory usage across all registered caches."""
        with self._lock:
            return sum(
                cache_info.get("memory_usage", 0) 
                for cache_info in self._cache_registry.values()
            )
    
    def get_cache_breakdown(self) -> dict:
        """Get detailed memory usage breakdown by cache type."""
        with self._lock:
            breakdown = {}
            
            for cache_id, cache_info in self._cache_registry.items():
                cache_type = cache_info.get("cache_type", "unknown")
                memory_usage = cache_info.get("memory_usage", 0)
                
                if cache_type not in breakdown:
                    breakdown[cache_type] = {
                        "total_memory": 0,
                        "cache_count": 0,
                        "caches": []
                    }
                
                breakdown[cache_type]["total_memory"] += memory_usage
                breakdown[cache_type]["cache_count"] += 1
                breakdown[cache_type]["caches"].append({
                    "cache_id": cache_id,
                    "memory_usage": memory_usage,
                    **cache_info
                })
            
            return breakdown
    
    def create_eviction_callback(self, cache_instance) -> callable:
        """Create an eviction callback for a specific cache instance."""
        def eviction_callback(target_reduction: float) -> int:
            """Eviction callback that frees memory from this cache."""
            try:
                current_usage = cache_instance.get_memory_usage()
                target_bytes = int(current_usage * target_reduction)
                
                bytes_freed = cache_instance.evict_to_target(target_bytes)
                
                # Update tracking
                new_usage = cache_instance.get_memory_usage() 
                self.update_cache_usage(
                    cache_instance.cache_id,
                    new_usage,
                    {"last_eviction": time.time(), "bytes_freed": bytes_freed}
                )
                
                return bytes_freed
                
            except Exception as e:
                logger.warning(f"Cache eviction failed for {cache_instance.cache_id}: {e}")
                return 0
        
        return eviction_callback
```

---

## Configuration System

### Memory Pressure Configuration

**Location**: `src/giflab/config.py`

```python
MONITORING = {
    "memory_pressure": {
        "enabled": True,                    # Enable memory pressure monitoring
        "update_interval": 5.0,             # Memory stats update interval (seconds)
        "auto_eviction": True,              # Enable automatic eviction
        "eviction_policy": "conservative",  # Eviction policy name
        
        # Memory pressure thresholds (percentage of system memory)
        "thresholds": {
            "warning": 0.70,    # 70% system memory usage
            "critical": 0.80,   # 80% system memory usage  
            "emergency": 0.95,  # 95% system memory usage
        },
        
        # Eviction targets (percentage of cache memory to free)
        "eviction_targets": {
            "warning": 0.15,    # Free 15% of cache memory
            "critical": 0.30,   # Free 30% of cache memory
            "emergency": 0.50,  # Free 50% of cache memory
        },
        
        # Hysteresis configuration (prevents oscillation)
        "hysteresis": {
            "enable_delta": 0.05,         # 5% threshold delta before eviction
            "eviction_cooldown": 30.0,    # 30 second cooldown between evictions
        },
        
        # Integration settings
        "alert_integration": {
            "enabled": True,
            "alert_levels": ["critical", "emergency"],
            "alert_cooldown": 300.0,      # 5 minute alert cooldown
        },
        
        # Performance settings
        "performance": {
            "max_collection_time_ms": 100.0,  # Max time for memory collection
            "enable_stats_caching": True,      # Enable memory stats caching
            "stats_cache_ttl": 5.0,           # Stats cache TTL (seconds)
        }
    }
}
```

### Configuration Profiles Integration

The memory monitoring system integrates with existing configuration profiles:

```python
# Development profile - more aggressive monitoring
"development": {
    "memory_pressure": {
        "update_interval": 2.0,        # More frequent updates
        "thresholds": {
            "warning": 0.60,           # Earlier warnings
            "critical": 0.75,
            "emergency": 0.90,
        }
    }
}

# Production profile - conservative settings
"production": {
    "memory_pressure": {
        "update_interval": 10.0,       # Less frequent updates
        "thresholds": {
            "warning": 0.75,           # Later warnings
            "critical": 0.85,
            "emergency": 0.95,
        }
    }
}

# Low memory profile - very aggressive
"low_memory": {
    "memory_pressure": {
        "thresholds": {
            "warning": 0.50,           # Very early warnings
            "critical": 0.65, 
            "emergency": 0.80,
        },
        "eviction_targets": {
            "warning": 0.25,           # More aggressive eviction
            "critical": 0.50,
            "emergency": 0.75,
        }
    }
}
```

---

## Integration Architecture

### 1. Alert Manager Integration

**Purpose**: Integrate memory pressure events with existing alerting infrastructure  
**Location**: `src/giflab/monitoring/memory_integration.py`

```python
class MemoryPressureIntegration:
    """Integration layer for memory pressure monitoring."""
    
    def __init__(self, config: dict, alert_manager=None):
        self.config = config
        self.alert_manager = alert_manager
        self._pressure_manager = MemoryPressureManager(config["memory_pressure"])
        self._cache_tracker = CacheMemoryTracker()
        
        # Set up automatic monitoring if enabled
        if config["memory_pressure"]["auto_eviction"]:
            self._start_pressure_monitoring()
    
    def _start_pressure_monitoring(self):
        """Start automatic pressure monitoring in background thread."""
        def monitor_loop():
            while True:
                try:
                    pressure_info = self._pressure_manager.trigger_eviction_if_needed()
                    
                    # Send alerts if configured
                    if (self.alert_manager and 
                        pressure_info["level"] in self.config["alert_integration"]["alert_levels"]):
                        self._send_memory_pressure_alert(pressure_info)
                    
                    # Log significant events
                    if pressure_info.get("evicted_bytes", 0) > 0:
                        logger.info(f"Memory pressure eviction: {pressure_info['level']}, "
                                  f"freed {pressure_info['evicted_bytes'] / 1024 / 1024:.1f}MB")
                    
                except Exception as e:
                    logger.error(f"Memory pressure monitoring error: {e}")
                
                time.sleep(self.config["memory_pressure"]["update_interval"])
        
        monitoring_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitoring_thread.start()
    
    def _send_memory_pressure_alert(self, pressure_info: dict):
        """Send memory pressure alert via alert manager."""
        alert_data = {
            "alert_type": "memory_pressure",
            "level": pressure_info["level"],
            "memory_percent": pressure_info["usage_percent"],
            "timestamp": time.time(),
            "details": {
                "total_memory": pressure_info["stats"].total_memory,
                "available_memory": pressure_info["stats"].available_memory,
                "process_memory": pressure_info["stats"].process_memory,
                "evicted_bytes": pressure_info.get("evicted_bytes", 0)
            }
        }
        
        try:
            self.alert_manager.send_alert(alert_data)
        except Exception as e:
            logger.warning(f"Failed to send memory pressure alert: {e}")
```

### 2. CLI Status Integration

**Purpose**: Provide real-time memory status through CLI commands  
**Location**: Enhanced in `src/giflab/cli/deps_cmd.py`

```python
def get_memory_status() -> dict:
    """Get comprehensive memory status for CLI display."""
    memory_monitor = SystemMemoryMonitor()
    stats = memory_monitor.get_memory_stats()
    
    # Get cache breakdown if available
    cache_breakdown = {}
    try:
        from giflab.monitoring.memory_integration import get_global_cache_tracker
        tracker = get_global_cache_tracker()
        if tracker:
            cache_breakdown = tracker.get_cache_breakdown()
    except ImportError:
        pass
    
    return {
        "system_memory": {
            "total_gb": stats.total_memory / (1024**3),
            "available_gb": stats.available_memory / (1024**3), 
            "usage_percent": stats.memory_percent * 100,
            "pressure_level": _determine_pressure_level(stats.memory_percent)
        },
        "process_memory": {
            "usage_mb": stats.process_memory / (1024**2),
            "percent_of_system": (stats.process_memory / stats.total_memory) * 100
        },
        "cache_memory": cache_breakdown,
        "timestamp": stats.timestamp
    }

@deps.command("status")
@click.option("--detailed", is_flag=True, help="Show detailed memory breakdown")
def memory_status(detailed: bool):
    """Show current memory usage and pressure status."""
    try:
        status = get_memory_status()
        
        # Quick status overview
        system = status["system_memory"]
        process = status["process_memory"]
        
        # Pressure level indicators
        pressure_indicators = {
            "normal": "ðŸŸ¢",
            "warning": "ðŸŸ¡", 
            "critical": "ðŸŸ ",
            "emergency": "ðŸ”´"
        }
        
        pressure_icon = pressure_indicators.get(system["pressure_level"], "â“")
        
        console.print(f"\n{pressure_icon} Memory Status: {system['pressure_level'].upper()}")
        console.print(f"System: {system['usage_percent']:.1f}% used "
                     f"({system['available_gb']:.1f}GB / {system['total_gb']:.1f}GB available)")
        console.print(f"Process: {process['usage_mb']:.1f}MB "
                     f"({process['percent_of_system']:.1f}% of system)")
        
        # Detailed breakdown if requested
        if detailed and status["cache_memory"]:
            console.print("\nðŸ“¦ Cache Memory Breakdown:")
            
            table = Table(title="Cache Memory Usage")
            table.add_column("Cache Type", style="cyan")
            table.add_column("Memory (MB)", justify="right", style="magenta") 
            table.add_column("Count", justify="right", style="green")
            table.add_column("Avg Size (MB)", justify="right", style="yellow")
            
            for cache_type, breakdown in status["cache_memory"].items():
                total_mb = breakdown["total_memory"] / (1024**2)
                count = breakdown["cache_count"]
                avg_mb = total_mb / count if count > 0 else 0
                
                table.add_row(
                    cache_type,
                    f"{total_mb:.1f}",
                    str(count),
                    f"{avg_mb:.1f}"
                )
            
            console.print(table)
        
    except Exception as e:
        console.print(f"âŒ Error getting memory status: {e}")
```

### 3. Metrics System Integration

**Purpose**: Feed memory statistics into existing metrics collection  
**Location**: `src/giflab/monitoring/memory_integration.py`

```python
def register_memory_metrics():
    """Register memory monitoring metrics with the metrics system."""
    
    from giflab.metrics_collector import MetricsCollector
    
    collector = MetricsCollector.get_instance()
    
    # System memory metrics
    collector.register_metric(
        "system_memory_usage_percent",
        description="System memory usage percentage",
        unit="percent",
        collection_func=lambda: _get_current_memory_stats().memory_percent * 100
    )
    
    collector.register_metric(
        "process_memory_usage_mb", 
        description="Process memory usage in MB",
        unit="megabytes",
        collection_func=lambda: _get_current_memory_stats().process_memory / (1024**2)
    )
    
    # Cache memory metrics
    collector.register_metric(
        "total_cache_memory_mb",
        description="Total cache memory usage in MB", 
        unit="megabytes",
        collection_func=lambda: _get_total_cache_memory() / (1024**2)
    )
    
    # Pressure events metrics
    collector.register_counter(
        "memory_pressure_events_total",
        description="Total memory pressure events by level",
        labels=["level"]  # warning, critical, emergency
    )
    
    collector.register_counter(
        "memory_evictions_bytes_total", 
        description="Total bytes evicted due to memory pressure",
        unit="bytes"
    )

def _get_current_memory_stats() -> MemoryStats:
    """Get current memory stats for metrics collection."""
    monitor = SystemMemoryMonitor()
    return monitor.get_memory_stats()

def _get_total_cache_memory() -> int:
    """Get total cache memory for metrics."""
    try:
        tracker = get_global_cache_tracker()
        return tracker.get_total_cache_memory() if tracker else 0
    except Exception:
        return 0
```

---

## Performance Analysis

### Monitoring Overhead

**Memory Collection Performance**:
```python
def benchmark_memory_collection():
    """Benchmark memory collection performance."""
    monitor = SystemMemoryMonitor()
    
    # Without caching
    times_uncached = []
    for _ in range(100):
        start = time.perf_counter()
        monitor._collect_memory_stats()
        end = time.perf_counter()
        times_uncached.append((end - start) * 1000)  # ms
    
    # With caching
    times_cached = []
    for _ in range(100):
        start = time.perf_counter()
        monitor.get_memory_stats()  # Uses caching
        end = time.perf_counter()
        times_cached.append((end - start) * 1000)  # ms
    
    return {
        "uncached_avg_ms": statistics.mean(times_uncached),
        "cached_avg_ms": statistics.mean(times_cached),
        "caching_speedup": statistics.mean(times_uncached) / statistics.mean(times_cached),
        "overhead_percent": (statistics.mean(times_cached) / 1000) * 100  # % of 1 second
    }
```

**Expected Performance**:
- **Memory Collection**: <5ms without caching, <0.5ms with caching  
- **Pressure Detection**: <1ms overhead for threshold checks
- **Eviction Callbacks**: Variable (depends on cache implementation)
- **Overall Monitoring**: <1% CPU overhead with 5-second intervals

### Memory Usage Impact

The monitoring system itself has minimal memory footprint:

- **MemoryStats Objects**: ~200 bytes each (dataclass with 5 fields)
- **Cache Registry**: ~1KB per registered cache (metadata only)
- **Configuration**: ~5KB for full configuration object
- **Thread Overhead**: ~8MB per monitoring thread (Python default)

**Total Monitoring Memory**: <10MB per process

---

## Error Handling and Recovery

### Graceful Degradation

```python
class SafeMemoryMonitor:
    """Memory monitor with comprehensive error handling."""
    
    def __init__(self, config: dict):
        self.config = config
        self._fallback_enabled = False
        
        try:
            self._monitor = SystemMemoryMonitor(config["update_interval"])
        except Exception as e:
            logger.warning(f"Failed to initialize memory monitor: {e}")
            self._enable_fallback_mode()
    
    def _enable_fallback_mode(self):
        """Enable fallback mode when monitoring unavailable."""
        self._fallback_enabled = True
        logger.info("Memory monitoring in fallback mode - limited functionality")
    
    def get_memory_stats(self) -> Optional[MemoryStats]:
        """Get memory stats with error handling."""
        if self._fallback_enabled:
            return self._get_fallback_stats()
        
        try:
            return self._monitor.get_memory_stats()
        except Exception as e:
            logger.warning(f"Memory stats collection failed: {e}")
            self._enable_fallback_mode()
            return self._get_fallback_stats()
    
    def _get_fallback_stats(self) -> MemoryStats:
        """Provide basic stats when monitoring fails."""
        return MemoryStats(
            total_memory=8 * 1024**3,    # Assume 8GB
            available_memory=2 * 1024**3, # Assume 2GB available  
            process_memory=100 * 1024**2, # Assume 100MB process
            memory_percent=0.75,          # Assume 75% usage
            timestamp=time.time()
        )

def emergency_disable_memory_monitoring():
    """Emergency procedure to disable memory monitoring."""
    try:
        # Stop monitoring threads
        import giflab.monitoring.memory_integration as integration
        integration.stop_all_monitoring()
        
        # Disable in configuration
        import giflab.config as config
        config.MONITORING["memory_pressure"]["enabled"] = False
        
        logger.warning("Memory monitoring disabled due to emergency")
        return True
        
    except Exception as e:
        logger.error(f"Failed to disable memory monitoring: {e}")
        return False
```

### Recovery Procedures

```python
def reset_memory_monitoring_state():
    """Reset memory monitoring to clean state."""
    
    # Clear any cached data
    import gc
    gc.collect()
    
    # Reinitialize monitoring components
    try:
        monitor = SystemMemoryMonitor()
        stats = monitor.get_memory_stats()
        
        if stats.memory_percent > 0.95:
            logger.warning("High memory usage detected during reset")
            return False
        
        logger.info("Memory monitoring state reset successfully")
        return True
        
    except Exception as e:
        logger.error(f"Memory monitoring reset failed: {e}")
        return False
```

---

## Testing and Validation

### Unit Test Patterns

```python
class TestMemoryMonitoring:
    """Comprehensive tests for memory monitoring system."""
    
    def test_memory_stats_collection(self):
        """Test basic memory statistics collection."""
        monitor = SystemMemoryMonitor()
        stats = monitor.get_memory_stats()
        
        assert isinstance(stats, MemoryStats)
        assert stats.total_memory > 0
        assert stats.available_memory > 0
        assert 0.0 <= stats.memory_percent <= 1.0
        assert stats.timestamp > 0
    
    def test_pressure_threshold_detection(self):
        """Test memory pressure threshold detection."""
        config = {
            "thresholds": {"warning": 0.7, "critical": 0.8, "emergency": 0.95},
            "eviction_targets": {"warning": 0.15, "critical": 0.3, "emergency": 0.5},
            "hysteresis": {"enable_delta": 0.05, "eviction_cooldown": 30.0}
        }
        
        manager = MemoryPressureManager(config)
        
        # Mock different pressure levels
        with patch.object(manager._memory_monitor, 'get_memory_stats') as mock_stats:
            # Test warning level
            mock_stats.return_value = MemoryStats(
                total_memory=8*1024**3,
                available_memory=2*1024**3, 
                process_memory=100*1024**2,
                memory_percent=0.75,  # 75% - warning level
                timestamp=time.time()
            )
            
            pressure = manager.check_memory_pressure()
            assert pressure["level"] == "warning"
            assert pressure["target_reduction"] == 0.15
    
    def test_eviction_callback_system(self):
        """Test cache eviction callback system."""
        tracker = CacheMemoryTracker()
        
        # Mock cache with eviction capability
        class MockCache:
            def __init__(self):
                self.cache_id = "test_cache"
                self.memory_usage = 100 * 1024 * 1024  # 100MB
            
            def get_memory_usage(self):
                return self.memory_usage
            
            def evict_to_target(self, target_bytes):
                evicted = min(target_bytes, self.memory_usage)
                self.memory_usage -= evicted
                return evicted
        
        mock_cache = MockCache()
        callback = tracker.create_eviction_callback(mock_cache)
        
        # Test eviction
        bytes_freed = callback(0.5)  # 50% reduction
        
        assert bytes_freed == 50 * 1024 * 1024  # Should free ~50MB
        assert mock_cache.memory_usage == 50 * 1024 * 1024  # 50MB remaining
    
    def test_hysteresis_prevention(self):
        """Test hysteresis prevention in eviction decisions."""
        config = {
            "hysteresis": {
                "enable_delta": 0.05,
                "eviction_cooldown": 10.0  # 10 second cooldown
            }
        }
        
        policy = ConservativeEvictionPolicy(config)
        
        # First eviction should be allowed
        assert policy.should_evict(0.80, 0.70)  # 10% above target
        
        # Simulate eviction occurred
        policy._last_eviction_time = time.time()
        
        # Second eviction should be blocked by cooldown
        assert not policy.should_evict(0.82, 0.70)  # Still above target but in cooldown
        
        # After cooldown, eviction should be allowed again
        policy._last_eviction_time = time.time() - 15.0  # 15 seconds ago
        assert policy.should_evict(0.82, 0.70)
```

### Integration Test Patterns

```python
def test_full_memory_pressure_workflow():
    """Test complete memory pressure detection and eviction workflow."""
    
    # Set up monitoring with test configuration
    test_config = {
        "memory_pressure": {
            "enabled": True,
            "update_interval": 0.1,  # Fast updates for testing
            "thresholds": {"warning": 0.01, "critical": 0.02, "emergency": 0.03},  # Very low for testing
            "eviction_targets": {"warning": 0.5, "critical": 0.7, "emergency": 0.9},
            "hysteresis": {"enable_delta": 0.001, "eviction_cooldown": 0.1}
        }
    }
    
    integration = MemoryPressureIntegration(test_config)
    
    # Create test cache that can be evicted
    test_cache = create_test_cache_with_data(size_mb=50)
    
    # Register cache with tracker
    callback = integration._cache_tracker.create_eviction_callback(test_cache)
    integration._pressure_manager.register_eviction_callback(callback)
    
    # Simulate high memory pressure
    with patch.object(integration._pressure_manager._memory_monitor, 'get_memory_stats') as mock_stats:
        mock_stats.return_value = MemoryStats(
            total_memory=1024**3,      # 1GB total
            available_memory=50*1024**2, # 50MB available  
            process_memory=974*1024**2,  # 974MB process (97.4% usage)
            memory_percent=0.974,       # Above emergency threshold
            timestamp=time.time()
        )
        
        # Trigger eviction
        result = integration._pressure_manager.trigger_eviction_if_needed()
        
        # Verify eviction occurred
        assert result["level"] == "emergency"
        assert result["evicted_bytes"] > 0
        assert test_cache.get_memory_usage() < 50 * 1024 * 1024  # Cache was evicted
```

---

## Best Practices and Guidelines

### Configuration Guidelines

1. **Conservative Thresholds**: Start with higher thresholds (75%/85%/95%) and adjust based on workload
2. **Appropriate Intervals**: Use longer intervals (5-10s) in production, shorter (1-2s) for development
3. **Hysteresis Settings**: Set cooldown periods based on cache rebuild time (30-60s typical)
4. **Eviction Targets**: Start conservative (10%/20%/40%) and increase if needed

### Performance Guidelines

1. **Monitor Overhead**: Keep total monitoring overhead <1% CPU usage
2. **Memory Collection**: Use caching to avoid frequent system calls
3. **Thread Safety**: Always use locks for shared memory state 
4. **Graceful Degradation**: Provide fallbacks when monitoring fails

### Integration Guidelines  

1. **Alert Integration**: Connect to existing alerting infrastructure
2. **Metrics Integration**: Feed into existing metrics collection systems
3. **CLI Integration**: Provide user-friendly status reporting
4. **Cache Integration**: Register all caches with memory tracker

---

## Summary

The memory monitoring infrastructure provides:

- **Proactive Protection**: Prevents memory exhaustion through early detection and automatic eviction
- **Production Safety**: Conservative defaults with comprehensive error handling
- **Cross-Platform Support**: Works across different operating systems with fallback implementations  
- **Performance Efficiency**: <1% monitoring overhead with configurable collection intervals
- **Complete Integration**: Seamless integration with alerting, metrics, and CLI systems
- **Thread Safety**: Safe for concurrent operations and multi-threaded processing

This infrastructure forms the foundation for safe memory management in the GifLab system, enabling reliable operation even under memory pressure conditions.

---

*Document Version: 1.0*  
*Last Updated: January 2025*  
*Related Documentation: [Conditional Import Architecture](conditional-import-architecture.md), [Configuration Guide](../configuration-guide.md), [CLI Dependency Management](../guides/cli-dependency-troubleshooting.md)*